{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "111dfc6a-f518-40bd-bd6f-73460d75bc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f02ee280-1a58-49ce-928a-b631ec780952",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, index = None, value = None, left = None, right = None, label = None):\n",
    "        self.index = index\n",
    "        self.value = value\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.leaf = leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e9a062a-c515-4763-95fc-b73fec31e9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    def __init__(self, max_depth = 5, min_group_size = 1, max_features = None, random_state = None): \n",
    "        self.root = None\n",
    "        self.max_depth = max_depth\n",
    "        self.min_group_size = min_group_size\n",
    "        self.max_features = max_features\n",
    "        self.random_state = random_state\n",
    "        \n",
    "        if self.random_state is not None:\n",
    "            random.seed(self.random_state)\n",
    "        \n",
    "    def _split(self, index, value, dataset):\n",
    "        left = []\n",
    "        right = []\n",
    "        for row in dataset:\n",
    "            if row[index] < value:\n",
    "                left.append(row)\n",
    "            else:\n",
    "                right.append(row)\n",
    "        return left, right\n",
    "\n",
    "    def _gini_score(self, dataset, classes):\n",
    "        size = len(dataset)\n",
    "        if size == 0:\n",
    "            return 0.0\n",
    "            \n",
    "        score  = 0.0\n",
    "        labels = [row[-1] for row in dataset]\n",
    "        \n",
    "        for cls in classes:\n",
    "            p = labels.count(cls)/size\n",
    "            score += p**2\n",
    "        return 1-score\n",
    "    \n",
    "    def _best_split(self, dataset):\n",
    "        if self.max_features is None:\n",
    "            self.max_features = len(dataset[0]) - 1\n",
    "\n",
    "        classes = list(set(row[-1] for row in dataset))\n",
    "        \n",
    "        best_index = None\n",
    "        best_value = None\n",
    "        best_gain = float('inf')\n",
    "        best_groups = None\n",
    "\n",
    "        features = random.sample(range(len(dataset[0])-1), min(len(dataset[0])-1, self.max_features))\n",
    "        \n",
    "        for index in features:\n",
    "            for row in dataset:\n",
    "                value = row[index]\n",
    "                left, right = self._split(index, value, dataset)\n",
    "                \n",
    "                left_gini = self._gini_score(left)\n",
    "                right_gini = self._gini_score(right)\n",
    "                \n",
    "                l = len(left)\n",
    "                r = len(right)\n",
    "                t = l+r\n",
    "                gain = left_gini*(l/t) +right_gini*(r/t)\n",
    "                \n",
    "                if gain < best_gain:\n",
    "                    \n",
    "                    best_index = index\n",
    "                    best_value = value\n",
    "                    best_gain = gain\n",
    "                    best_groups = (left, right)\n",
    "                    \n",
    "        return best_index, best_value, best_groups\n",
    "\n",
    "    def _leaf(self, dataset):\n",
    "        classes = list(set([row[-1] for row in dataset]))\n",
    "        labels = {cls: 0 for cls in classes}\n",
    "\n",
    "        for row in dataset:\n",
    "            labels[row[-1]] += 1\n",
    "\n",
    "        most_common_label = None\n",
    "        max_count = 0\n",
    "\n",
    "        for label, count in labels.items():\n",
    "            if count > max_count:\n",
    "                max_count = count\n",
    "                most_common_label = label\n",
    "        \n",
    "        return most_common_label\n",
    "\n",
    "    def _build_tree(self, dataset, depth):\n",
    "        index, value, groups = self._best_split(dataset)\n",
    "        if not groups or index is None:\n",
    "            return Node(label=self._leaf(dataset))\n",
    "        left, right = groups\n",
    "        node = Node(index = index, value = value)\n",
    "\n",
    "        if depth >= self.max_depth:\n",
    "            node.left = Node(label = self._leaf(left))\n",
    "            node.right = Node(label = self._leaf(right))\n",
    "            return node\n",
    "            \n",
    "        if len(left) <= self.min_group_size:\n",
    "            node.left = Node(label = self._leaf(left))\n",
    "        else:\n",
    "            node.left = self._build_tree(left, depth + 1)\n",
    "\n",
    "        if len(right) <= self.min_group_size:\n",
    "            node.right = Node(label = self._leaf(right))\n",
    "        else:\n",
    "            node.right = self._build_tree(right, depth + 1)\n",
    "\n",
    "        return node\n",
    "\n",
    "    def _predict(self, node, row):\n",
    "        if node.label is not None:\n",
    "            return node.label\n",
    "            \n",
    "        if row[node.index] < node.value:\n",
    "            return self._predict(node.left, row)\n",
    "        else:\n",
    "            return self._predict(node.right, row)\n",
    "    \n",
    "    def predict(self, row):\n",
    "        return self._predict(self.root, row)\n",
    "\n",
    "    def predict_all(self, data):\n",
    "        return [self.predict(row) for row in data]\n",
    "\n",
    "    \n",
    "    def fit(self, dataset):\n",
    "        self.root = self._build_tree(dataset, depth = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee84bef-0d0b-4ad3-9bb7-d3d8a1342cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForest:\n",
    "    def __init__(self, num_trees = 10, max_features = None, random_state = None):\n",
    "        self.num_trees = num_trees\n",
    "        self.max_features = max_features\n",
    "        self.random_state = random_state\n",
    "        self.trees = []\n",
    "\n",
    "        if self.random_state is not None:\n",
    "            random.seed(self.random_state)\n",
    "\n",
    "    def _bootstrap_sample(self, data):\n",
    "        n = len(data)\n",
    "        return [random.choice(data) for _ in range(n)]\n",
    "\n",
    "    def fit(self, data):\n",
    "        self.trees = []\n",
    "        \n",
    "        for i in range(self.num_trees):\n",
    "            sample = self._bootstrap_sample(data)\n",
    "\n",
    "            tree_seed = None\n",
    "            if self.random_state is not None:\n",
    "                tree_seed = self.random_state + i\n",
    "                \n",
    "            tree = DecisionTree(\n",
    "                max_features=self.max_features\n",
    "                random_state = tree_seed\n",
    "            )\n",
    "            tree.fit(sample)\n",
    "            self.trees.append(tree)\n",
    "\n",
    "    def predict(self, row):\n",
    "        predictions = [tree.predict(row) for tree in self.trees]\n",
    "        predict_count = { prediction : 0 for prediction in set(predictions) }\n",
    "        for prediction in predictions:\n",
    "            predict_count[prediction] += 1\n",
    "\n",
    "        most_common_prediction = None\n",
    "        max_count = 0\n",
    "\n",
    "        for prediction, count in predict_count.items():\n",
    "            if count > max_count:\n",
    "                max_count = count\n",
    "                most_common_prediction = prediction\n",
    "        return most_common_prediction\n",
    "\n",
    "    def predict_batch(self, rows):\n",
    "        return [self.predict(row) for row in rows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5329d869-759b-4438-a797-1774176da01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaBoost:\n",
    "    def __init__(self, n_estimators=50, max_depth=1, min_group_size=1, max_features=None, random_state=None):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.min_group_size = min_group_size\n",
    "        self.max_features = max_features\n",
    "        self.random_state = random_state\n",
    "        self.models = []\n",
    "        self.alphas = []\n",
    "\n",
    "        if self.random_state is not None:\n",
    "            random.seed(self.random_state)\n",
    "\n",
    "    def _weighted_sample(self, data, weights):\n",
    "        n = len(data)\n",
    "        sampled_data = random.choices(data, weights=weights, k=n)\n",
    "        return sampled_data\n",
    "\n",
    "    def fit(self, data):\n",
    "        n = len(data)\n",
    "        weights = [1/n] * n\n",
    "\n",
    "        for m in range(self.n_estimators):            \n",
    "            sample = self._weighted_sample(data, weights)\n",
    "\n",
    "            tree = DecisionTree(\n",
    "                max_depth=self.max_depth,\n",
    "                min_group_size=self.min_group_size,\n",
    "                max_features=self.max_features,\n",
    "                random_state=self.random_state\n",
    "            )\n",
    "            tree.fit(sample)\n",
    "\n",
    "            # Compute weighted error\n",
    "            error = 0.0\n",
    "            for i, row in enumerate(data):\n",
    "                prediction = tree.predict(row)\n",
    "                if prediction != row[-1]:\n",
    "                    error += weights[i]\n",
    "\n",
    "            # Avoiding divide-by-zero or unusable weak learner\n",
    "            if error > 0.5 or error == 0:\n",
    "                continue\n",
    "\n",
    "            # Computing alpha (learner weight)\n",
    "            alpha = 0.5 * math.log((1 - error) / error)\n",
    "\n",
    "            # Update sample weights\n",
    "            for i, row in enumerate(data):\n",
    "                actual = row[-1]\n",
    "                pred = tree.predict(row)\n",
    "                weights[i] *= math.exp(-alpha * actual * pred)\n",
    "\n",
    "            # Normalize weights\n",
    "            total_weight = sum(weights)\n",
    "            weights = [w / total_weight for w in weights]\n",
    "\n",
    "            # Save model and its alpha\n",
    "            self.models.append(tree)\n",
    "            self.alphas.append(alpha)\n",
    "\n",
    "    def predict(self, row):\n",
    "        total = 0.0\n",
    "        for alpha, model in zip(self.alphas, self.models):\n",
    "            pred = model.predict(row)\n",
    "            total += alpha * pred\n",
    "        return 1 if total >= 0 else -1\n",
    "\n",
    "    def predict_batch(self, data):\n",
    "        return [self.predict(row) for row in data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aec05c5-edc7-4f88-9881-ebf449a23462",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layers, learning_rate):\n",
    "      \n",
    "        self.num_layers = len(layers)\n",
    "        self.layers = layers\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "\n",
    "        for i in range(self.num_layers - 1):\n",
    "            weight = np.random.randn(layers[i+1], layers[i]) * 0.01\n",
    "            bias = np.zeros((layers[i+1], 1))\n",
    "            self.weights.append(weight)\n",
    "            self.biases.append(bias)\n",
    "\n",
    "    def tanh(self, z):\n",
    "        return np.tanh(z)\n",
    "\n",
    "    def d_tanh(self, z):\n",
    "        return 1 - np.tanh(z)**2\n",
    "\n",
    "    def relu(self, z):\n",
    "        return np.maximum(0, z)\n",
    "\n",
    "    def relu_derivative(self, z):\n",
    "        return (z > 0).astype(float)\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def d_sigmoid(self, z):\n",
    "        s = self.sigmoid(z)\n",
    "        return s * (1 - s)\n",
    "\n",
    "    def forward(self, x):\n",
    "        activations = [x]\n",
    "        zs = []\n",
    "\n",
    "        a = x\n",
    "        for i in range(self.num_layers - 1):\n",
    "            z = np.dot(self.weights[i], a) + self.biases[i] # starting from 1st hidden layer to last layer\n",
    "            zs.append(z)\n",
    "\n",
    "            if i == 0:\n",
    "                a = self.tanh(z) # for the first hidden layer\n",
    "            elif i == self.num_layers - 2:\n",
    "                a = self.sigmoid(z) # for the last layer/ output layer\n",
    "            else:\n",
    "                a = self.relu(z) # for all the hidden layers except the first\n",
    "\n",
    "            activations.append(a)\n",
    "\n",
    "        return activations[-1], (activations, zs)\n",
    "\n",
    "    def backward(self, x, y, activations, zs, learning_rate):\n",
    "        grads_w = [0] * (self.num_layers - 1)\n",
    "        grads_b = [0] * (self.num_layers - 1)\n",
    "\n",
    "        # Output layer error\n",
    "        delta = (activations[-1] - y) * self.sigmoid_derivative(zs[-1])\n",
    "        grads_w[-1] = np.dot(delta, activations[-2].T)\n",
    "        grads_b[-1] = delta\n",
    "\n",
    "        # Backpropagate\n",
    "        for l in range(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            if l == self.num_layers - 1:\n",
    "                sp = self.tanh_derivative(z)\n",
    "            else:\n",
    "                sp = self.relu_derivative(z)\n",
    "\n",
    "            delta = np.dot(self.weights[-l+1].T, delta) * sp\n",
    "            grads_w[-l] = np.dot(delta, activations[-l-1].T)\n",
    "            grads_b[-l] = delta\n",
    "\n",
    "        # Update weights and biases\n",
    "        for i in range(self.num_layers - 1):\n",
    "            self.weights[i] -= learning_rate * grads_w[i]\n",
    "            self.biases[i] -= learning_rate * grads_b[i]\n",
    "\n",
    "    def train(self, X, Y, epochs=1000, learning_rate=0.01):\n",
    "        \"\"\"\n",
    "        X: numpy array of shape (num_samples, input_size)\n",
    "        Y: numpy array of shape (num_samples, output_size)\n",
    "        \"\"\"\n",
    "        for epoch in range(epochs):\n",
    "            for i in range(X.shape[0]):\n",
    "                x = X[i].reshape(-1, 1)\n",
    "                y = Y[i].reshape(-1, 1)\n",
    "                output, (activations, zs) = self.forward(x)\n",
    "                self.backward(x, y, activations, zs, learning_rate)\n",
    "\n",
    "            if epoch % 100 == 0:\n",
    "                loss = self.compute_loss(X, Y)\n",
    "                print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "\n",
    "    def compute_loss(self, X, Y):\n",
    "        total_loss = 0\n",
    "        for i in range(X.shape[0]):\n",
    "            x = X[i].reshape(-1, 1)\n",
    "            y = Y[i].reshape(-1, 1)\n",
    "            output, _ = self.forward(x)\n",
    "            total_loss += np.sum((output - y) ** 2)\n",
    "        return total_loss / X.shape[0]\n",
    "\n",
    "    def predict(self, X):\n",
    "        preds = []\n",
    "        for i in range(X.shape[0]):\n",
    "            x = X[i].reshape(-1, 1)\n",
    "            output, _ = self.forward(x)\n",
    "            preds.append(output)\n",
    "        return np.array(preds).squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44d79a2-703f-48ed-9c44-3525302cc124",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, layers, data, activations = None,label = None):\n",
    "        self.n_layers = len(layers)-1\n",
    "        self.layers = layers\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "\n",
    "        if activations is None:\n",
    "            raise ValueError(\"Can not be None\")\n",
    "        else:\n",
    "            self.activations = activations\n",
    "            \n",
    "        if label is None:\n",
    "            self.label = np.array([row[-1] for row in data])\n",
    "            self.data = np.array([row[:-1] for row in data]).T\n",
    "        else:\n",
    "            self.label = label\n",
    "            self.data = data\n",
    "\n",
    "        for i in range(self.n_layers):\n",
    "            weight = np.random.randn(self.layers[i+1], self.layers[i])*0.01\n",
    "            bias = np.zeros((self.layers[i+1],1))\n",
    "            self.weights.append(weight)\n",
    "            self.biases.append(bias)\n",
    "\n",
    "    def tanh(self, x):\n",
    "        return np.tanh(x)\n",
    "    def d_tanh(self, x):\n",
    "        return 1- np.tanh(x)**2\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1/(1+np.exp(-x))\n",
    "    def d_sigmoid(self, x):\n",
    "        s = self.sigmoid(x)\n",
    "        return s*(1-s)\n",
    "\n",
    "    def ReLU(self, x):\n",
    "        return np.maximum(x, 0)\n",
    "    def d_ReLU(self, x):\n",
    "        return (x > 0).astype(float)\n",
    "\n",
    "    def leaky_ReLU(self, x):\n",
    "        return np.maximum(x, 0.01*x)\n",
    "    def d_leaky_ReLU(self, x):\n",
    "        return np.where(x > 0, 1, 0.01)\n",
    "\n",
    "    def forward(self, X):\n",
    "        Z = []\n",
    "        A = []\n",
    "        a = X\n",
    "        for i in range(self.n_layers):\n",
    "            z= np.dot(self.weights[i], a) + self.biases[i]\n",
    "            activation = self.activations[i]\n",
    "            match activation:\n",
    "                case \"tanh\":\n",
    "                    a = self.tanh(z)\n",
    "                case \"sigmoid\":\n",
    "                    a = self.sigmoid(z)\n",
    "                case \"relu\":\n",
    "                    a = self.ReLU(z)\n",
    "                case \"leaky_relu\":\n",
    "                    a = self.leaky_ReLU(z)\n",
    "                \n",
    "            Z.append(z)\n",
    "            A.append(a)\n",
    "            \n",
    "        return a, Z, A\n",
    "        \n",
    "    def backward(self, Z, A, Y):\n",
    "        dW  = [0]*(self.n_layers)\n",
    "        dB  = [0]*(self.n_layers)\n",
    "        m = self.data.shape[1]\n",
    "        X = self.data\n",
    "        # Output layer, \n",
    "        dz = A[-1]- Y\n",
    "        # gradient claculation\n",
    "        for i in range(self.n_layers-1, -1, -1):\n",
    "            if i == 0:\n",
    "                dw = np.dot(dz, X.T)/m\n",
    "            else:\n",
    "                dw = np.dot(dz, A[i-1].T)/m\n",
    "                \n",
    "            db = np.sum(dz, axis =1 ,keepdims =True)/m\n",
    "            \n",
    "            if i > 0:\n",
    "                activation = self.activations[i - 1]\n",
    "                match activation:\n",
    "                    case \"tanh\":\n",
    "                        dz = np.dot(self.weights[i].T, dz) * self.d_tanh(Z[i - 1])\n",
    "                    case \"relu\":\n",
    "                        dz = np.dot(self.weights[i].T, dz) * self.d_ReLU(Z[i - 1])\n",
    "                    case \"sigmoid\":\n",
    "                        dz = np.dot(self.weights[i].T, dz) * self.d_sigmoid(Z[i - 1])\n",
    "                    case \"leaky_relu\":\n",
    "                        dz = np.dot(self.weights[i].T, dz) * self.d_leaky_ReLU(Z[i - 1])\n",
    "                    case _:\n",
    "                        raise ValueError(f\"Unknown activation '{activation}' at layer {i - 1}\")\n",
    "\n",
    "            dW[i] = dw\n",
    "            dB[i] = db\n",
    "        \n",
    "        return dW, dB\n",
    "\n",
    "    def update(self, dW, dB, learning_rate):\n",
    "        \n",
    "        for i in range(self.n_layers):\n",
    "            self.weights[i] = self.weights[i] - learning_rate * dW[i]\n",
    "            self.biases[i] = self.biases[i] - learning_rate * dB[i]\n",
    "        \n",
    "        return\n",
    "\n",
    "    def loss (self, X, Y):\n",
    "        m = X.shape[1]\n",
    "\n",
    "        output, _, _ = self.forward(X)\n",
    "        epsilon = 1e-8\n",
    "        computed_loss = -(Y*np.log(output + epsilon) + (1-Y)*np.log(1-output + epsilon))\n",
    "        loss = np.sum(computed_loss)/m\n",
    "        return loss\n",
    "    \n",
    "    def predict(self, data):\n",
    "        data = np.array(data).T\n",
    "        probability, _, _ = self.forward(data)\n",
    "        prediction = (probability > 0.5).astype(float)\n",
    "        return prediction\n",
    "    \n",
    "    def accuracy(self, data, true_label):\n",
    "        prediction = self.predict(data)\n",
    "        Y = np.array(true_label)\n",
    "        accuracy = np.mean(prediction == Y) * 100\n",
    "        return accuracy\n",
    "    def _accuracy(self, output):\n",
    "        Y = self.label\n",
    "        prediction = (output > 0.5).astype(float)\n",
    "        accuracy = np.mean(prediction == Y) * 100\n",
    "        return accuracy\n",
    "    \n",
    "    def train(self, epochs = 1000, learning_rate = 0.01):\n",
    "        X = self.data\n",
    "        Y = self.label\n",
    "        print(\"Epoch\\t|\\tloss\\t|\\tAccuracy\")\n",
    "        for epoch in range(epochs):\n",
    "            output, Z, A = self.forward(X)\n",
    "            dW, dB = self.backward(Z, A, Y)\n",
    "            self.update(dW, dB, learning_rate)\n",
    "            loss = self.loss(X,Y)\n",
    "            accuracy = self._accuracy(output)\n",
    "            print(f\"{epoch+1}/{epochs}\\t|\\t{loss:.4f}\\t|\\t{accuracy:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
